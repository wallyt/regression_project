---
title: "MotorTrend MPG Regression Analysis"
author: "Wally Thornton"
date: "September 27, 2015"
output: pdf_document
---
#MotorTrend MPG Regression Analysis
In this project, we are asked by the editor of MotorTrend magazine to answer two questions: 1. Is an automatic or manual transmission better for mpg, and 2. Quantify the mpg difference between automatic and manual transmissions. As will be shown in this knitr document, the simple answers are manual and 7.24 mpg. But the more complex answer is, transmission type is a poor predictor of mpg, and the magazine would better serve its readers with an article about weight and horsepower predicting mpg.
```{r simSetup, message=FALSE, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(fig.width=9)
options(scipen=999)
setwd("~/Documents/DataScience/Regression Models")
# ensurePkg tests whether the package is installed and, if not, installs it.
ensurePkg <- function(x) {
    if (!require(x,character.only = TRUE)) {
        install.packages(x,dep=TRUE, repos="http://cran.r-project.org")
        if(!require(x,character.only = TRUE)) stop("Package not found")
    }
}
ensurePkg('scales')
ensurePkg('corrplot')
#ensurePkg('tidyr')
ensurePkg('dplyr')
ensurePkg('ggplot2')
data("mtcars")
```

The `mtcars` dataset has `r nrow(mtcars)` cars of myriad makes and models, with a mean mpg of `r mean(mtcars$mpg)` and standard deviation of mpg as `r sd(mtcars$mpg)`, so there's quite a bit of variance in the values (a histogram of the values for mpg is in the Appendix, Figure 1).

The `mtcars` description file defines `mpg` as miles/US gallon and `am` as the transmission type, with 0 signifying automatic and 1 manual. Since we are examining the influence (or lack thereof) of the transmission type on miles per gallon, let's look at the breakdown of the two types in the data set:
```{r echo=FALSE}
table(mtcars$am, dnn = "0=Auto, 1=Manual")
```
So `r percent(sum(mtcars$am == 0)/length(mtcars$am))` of the cars have automatic transmissions. A density plot in the Appendix (Figure 2) shows that there is a noticeable difference in mpg by transmission type, which the correlation between the two variables confirms: `r cor(mtcars$am, mtcars$mpg)`. While this correlation is not strong enough to convince us, the magazine editor believes transmission type is causal, so we run our first model with a linear regression, using `am` as the predictor and `mpg` as the outcome, resulting in:
```{r echo=FALSE}
fit <- lm(mpg ~ am, data = mtcars)
fitSum <- summary(fit)
fitResid <- resid(fit)
fitSum$coef
```
$\beta_0$ (or the y intercept) is `r round(fitSum$coef[1,1],2)` while $\beta_1$ (or the slope of x) is `r round(fitSum$coef[2,1],2)`. Since automatic transmission in this dataset is set to the value 0 in `am`, $\beta_0$ is the mean mpg for an automatic transmission and $\beta_1$ is the predicted gain in MPG for the manual transmission cars.

With the p-value of the slope well below our pre-selected $\alpha$ of 0.05, we could conclude that the transmission type does affect mpg, with the move from automatic to manual increasing mpg by 7.24. (A scatterplot with fitted regression line is shown in Appendix, Figure 3.) However, $R^2$ is `r round(fitSum$adj.r.squared,2)`, which means that this model only explains `r percent(round(fitSum$adj.r.squared,2))` of the variance of the data. This is evident when we plot the residuals (shown in Appendix, Figure 4), and see the broad dispersion, although they are nicely distributed on both sides of the regression line. Relatedly, the standard error of this model is `r round(fitSum$sigma,2)`, which is quite high for this dataset. We have confounders to find.

With multiple potentially important variables in the dataset (and with many perhaps derivatives of others, such as `qsec`), we run a correlation matrix first to narrow down our choices (in Appendix, Figure 5). The results show that while `am` has a very loose correlation with `mpg` (`r round(cor(mtcars$am,mtcars$mpg),2)`), other variables are much more correlated (e.g., `wt`: `r round(cor(mtcars$mpg, mtcars$wt),2)`). We also see that some predictors exhibit high colinearity (and are logically connected, like displacement and number of cylinders).

We want to be parsimonious with our model while returning the best-performing model, so we'll pursue three paths. First, a couple single-predictor models using the highest-correlating variables. Second, running a model with all predictors and iteratively removing the variables with highest p-values. And third, our hypothesis that `wt` and `hp`  so we'll first look at just `wt` as a predictor and then run the model with all predictors, followed by systematically removing those with high p-values, with an eye toward increasing $R^2$ while keeping RSE as low as possible.

#TODO: ADD SOME LOGIC AS TO WHY I'M CHOOSING CERTAIN VARIABLES
```{r echo=FALSE}
fit2 <- lm(mpg ~ wt, data=mtcars)
fit2Sum <- summary(fit2)
fit2Sum$coef

fit3 <- lm(mpg ~ disp, data=mtcars)
fit3Sum <- summary(fit3)
fit3Sum$Coef

fit4 <- lm(mpg ~ wt + disp + hp + am, data=mtcars)
fit4Sum <- summary(fit4)
fit4Sum$coef
```

Weight, by itself, is a much better predictor than `am` alone, with `mpg` declining 5.3 for every With an adjusted $R^2$ of `r round(fit2Sum$adj.r.squared, 2)`, this leaves 26% of the variance unexplained, which is much better than using `am` alone, but not as good as the model with additional predictors with an $R^2$ of `r round(fit3Sum$adj.r.squared, 2)` and RSE of only `r round(fit2Sum$sigma, 2)`. But even this can be improved, since hp is a function of displacement, cylinders, gearing and other confounding factors that aren't in the dataset. Horsepower and weight appear to have the greatest influence on mileage, so our final model focuses on the interaction between these two variables:
```{r echo=FALSE}
fit4 <- lm(mpg ~ wt * hp, data = mtcars)
fit4Sum <- summary(fit4)
fit4Sum$coef
```

The adjusted $R^2$ of this last model is `r round(fit4Sum$adj.r.squared, 2)`, which is pretty good for this dataset and has a residual standard error of only `r round(fit4Sum$sigma, 2)`.

#TODO: come to conclusions, ensure initial questions are explicitly answered

-------------------
#Appendix
```{r mpg_hist, echo=F, message=F}
g <- ggplot(mtcars, aes(mtcars$mpg)) + 
    geom_histogram(binwidth=1, fill="steelblue", color="grey") + 
    ggtitle("Fig 1: Count of MPG Values of Cars") + 
    xlab("MPG")
g
```

```{r mpg_am_box, echo=F, message=F}
g <- ggplot(mtcars, aes(x=mpg, fill=as.factor(am))) + 
    geom_density(alpha=.3) + 
    ggtitle("Fig 2: Densities of MPG Values by Transmission Type") + 
    xlab("MPG")
g
```

```{r fit_lm, echo=F, message=F}
g = ggplot(mtcars, aes(x = am, y = mpg)) + 
    geom_smooth(method = "lm", colour = "black") + 
    geom_point(size = 7, color = "grey") + 
    geom_point(size = 5, color = "steelblue", alpha = 0.7) + 
    ggtitle("Fig 3: Linear Regression Line for MPG Predicted by Transmission Type") + 
    xlab("Transmission Type") + 
    ylab("MPG")
g
```

```{r fit_resid, echo=F, message=F}
g = ggplot(data.frame(x = mtcars$am, y = resid(lm(mtcars$mpg ~ mtcars$am))), 
           aes(x = x, y = y)) + 
    geom_hline(yintercept = 0, size = 2) + 
    geom_point(size = 7, color = "grey") + 
    geom_point(size = 5, color = "steelblue", alpha = 0.7) + 
    ggtitle("Fig 4: Plot of mpg~am Residuals") + 
    xlab("X") + ylab("Residual")
g
```

##Fig 5: Correlation Matrix of All Factors
```{r corMat, echo=F, message=F}
cor <- cor(mtcars)
corrplot.mixed(cor, order="AOE")
```
